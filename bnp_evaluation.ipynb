{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### This notebook contains the codes for data preprocessing and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model \n",
    "from itertools import combinations\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test= pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114321 observations and 131 features\n"
     ]
    }
   ],
   "source": [
    "print(\"{} observations and {} features\".format(train.shape[0], train.shape[1] - 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select variables with higher correlations against the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var = ['target', 'v10', 'v12', 'v14', 'v21', 'v22', 'v24', 'v30', 'v31', 'v34', 'v38', 'v40', 'v47', 'v50', 'v52', 'v56', 'v62', \n",
    "'v66', 'v72', 'v74', 'v75', 'v79', 'v107','v112', 'v113', 'v114', 'v129']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 duplicates\n"
     ]
    }
   ],
   "source": [
    "print('%d duplicates' % sum(train.duplicated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select the variables with higher correlations against the target variable\n",
    "train_X = train.loc[:,var]\n",
    "train_y = train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat_cols = train_X.select_dtypes(include=['O']).columns\n",
    "num_cols = train_X.select_dtypes(include=['int', 'float']).columns[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing  with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_X.loc[:,num_cols]=train_X.loc[:,num_cols].fillna(-1) # Fill NaN with -1\n",
    "train_X.loc[:,cat_cols]=train_X.loc[:,cat_cols].fillna('99') # Fill NaN with '99' for categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting some numerical variables back to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X.loc[:,'v10'] = round(train_X.loc[:,'v10'] / 0.02188170970971, 2)\n",
    "train_X.loc[:,'v40'] = round(train_X.loc[:,'v40'] / 0.000723536569601, 2)\n",
    "train_X.loc[:,'v50'] = round(train_X.loc[:,'v50'] / 0.00146724324324, 2)\n",
    "train_X = round(train_X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating new variables for capturing the interactions among categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in cat_cols[1:]:\n",
    "    train.loc[:, 'v22'+c] = train_X.loc[:, 'v22'] + train_X.loc[:, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in combinations(cat_cols, 2):\n",
    "    train_X.loc[:,c[0]+c[1]] = train_X.loc[:,c[0]] + train_X.loc[:,c[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in combinations(cat_cols[1:], 2):\n",
    "    train_X.loc[:,'v22'+c[0]+c[1]] = train_X.loc[:,'v22'] + train_X.loc[:,c[0]] + train_X.loc[:,c[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X.loc[:, 'v22'+'v40-v50'] = train_X.loc[:, 'v22'] + (train_X.loc[:,'v40']-train_X.loc[:,'v50']).map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a training set and an evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_X, eval_X, train_y, eval_y = train_test_split(train_X, train_y, test_size=0.25, train_size=0.75, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Convert v40 and v50 back to integers by dividing a denominator and deduct v50 from v40\n",
    "# train_X['v40-v50'] = round(train_X['v40']/0.000723536569601, 2) - round(train_X['v50']/0.00146724324324, 2)\n",
    "# eval_X['v40-v50'] = round(eval_X['v40']/0.000723536569601, 2) - round(eval_X['v50']/0.00146724324324, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (To be removed) Estimate the target variable for the evaluation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'categorical_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-681bcf986e50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcols_with_same_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategorical_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mcols_with_same_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcols_with_same_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'categorical_cols' is not defined"
     ]
    }
   ],
   "source": [
    "cols_with_same_values = []\n",
    "for col in categorical_cols:\n",
    "    if set(train_X[col].unique()) == set(eval_X[col].unique()):\n",
    "        cols_with_same_values.append(col)\n",
    "cols_with_same_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 1.5\n",
    "f = 0.5\n",
    "pt = sum(train_X.target)/len(train_X) # the prior probability of the dependent attribute\n",
    "def beta(x, t_count, k, f):\n",
    "    ni = t_count[x]\n",
    "    return 1 / (1 + np.exp(-(ni-k)/f))\n",
    "\n",
    "def transform_to_scalar(x, t_mean, pt, beta):\n",
    "    pi = t_mean[x]\n",
    "    return beta * pi + (1 - beta) * pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in cols_with_same_values:\n",
    "    t_mean = train_X.groupby([col])['target'].mean()\n",
    "    t_count = train_X.groupby([col])['target'].count()\n",
    "    train_X[col] = train_X[col].apply(lambda x: transform_to_scalar(x, t_mean, pt, beta(x, t_count, k, f)))\n",
    "    eval_X[col] = eval_X[col].apply(lambda x: transform_to_scalar(x, t_mean, pt, beta(x, t_count, k, f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ext = ensemble.ExtraTreesClassifier(n_estimators=1200, max_features= 8, criterion= 'entropy', min_samples_split= 2,\n",
    "                            max_depth= 30, min_samples_leaf= 2, n_jobs = -1, random_state=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols_to_train = list(num_cols) + cols_with_same_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
       "           max_depth=30, max_features=30, max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=2,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=1200, n_jobs=-1, oob_score=False, random_state=1,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ext.fit(train_X[cols_to_train], train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_X['target'] = ext.predict(eval_X[cols_to_train]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Categorical Encoding Using Target Statistics \n",
    "Transform individual value of a categorical attribute $X$ to a scalar, $S_i$, representing the estimate of probability that the target variable equals to 1 given $X=X_i$. $S_i$ is calculated using the formula: $$S_i=\\beta(n_i)\\frac{n_{iy}}{n_i}+(1-\\beta(n_i))\\frac{n_y}{n_{TR}}$$ \n",
    "where $n_y$ is total number of cases such that $y=1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "k = 1.5\n",
    "f = 0.5\n",
    "pt = sum(train_X.target)/len(train_X) # the prior probability of the dependent attribute\n",
    "def beta(x, count, k, f):\n",
    "    ni = 0 if x not in count else count[x]\n",
    "    return 1 / (1 + np.exp(-(ni-k)/f))\n",
    "\n",
    "def transform_to_scalar(x, mean, pt, beta):\n",
    "    pi = pt if x not in mean else mean[x]\n",
    "    return (beta * pi + (1 - beta) * pt) # * np.random.normal(1, 0.03)\n",
    "\n",
    "def beta_(x, count, k, f):\n",
    "    ni = 0 if (x[0], x[1]) not in count else count.loc[x[0], x[1]]\n",
    "    return 1 / (1 + np.exp(-(ni-k)/f))\n",
    "\n",
    "def transform_to_scalar_(x, mean, pt, beta):\n",
    "    pi = pt if (x[0], x[1]) not in mean else mean.loc[x[0], x[1]]\n",
    "    return (beta * pi + (1 - beta) * pt) # * np.random.normal(1, 0.03)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-One-Out Encoding Using Target Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols = train_X.select_dtypes(include=['O']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_rows = train_X.shape[0]\n",
    "train_X = train_X.reset_index(drop=True)\n",
    "train_y = train_y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    mean = train_X.groupby([col])['target'].mean()\n",
    "    count = train_X.groupby([col])['target'].count()\n",
    "    eval_X.loc[:, col] = eval_X.loc[:,col].apply(lambda x: transform_to_scalar(x, mean, pt, beta(x, count, k, f)))\n",
    "    for i in range(5):\n",
    "        fold = range(i, n_rows, 5)\n",
    "        mean = train_X.drop(fold, axis=0).groupby([col])['target'].mean()\n",
    "        count = train_X.drop(fold, axis=0).groupby([col])['target'].count()\n",
    "        train_X.loc[fold, col] = train_X.loc[fold,col].apply(lambda x: transform_to_scalar(x, mean, pt, beta(x, count, k, f)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X = train_X.drop('target', axis=1)\n",
    "eval_X = eval_X.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X[cat_cols] = train_X[cat_cols].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "XGB = XGBClassifier(learning_rate =0.01, n_estimators=2000, max_depth=8,\n",
    " min_child_weight=1.5, gamma=0, reg_alpha=0.01, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic',\n",
    " nthread=4, scale_pos_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0, learning_rate=0.01, max_delta_step=0, max_depth=8,\n",
       "       min_child_weight=1.5, missing=None, n_estimators=2000, nthread=4,\n",
       "       objective='binary:logistic', reg_alpha=0.01, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB.fit(train_X, train_y, eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prob_xgb = XGB.predict_proba(eval_X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.71808976,  0.38508865,  0.54427785, ...,  0.9025293 ,\n",
       "        0.67543477,  0.97460437], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.443116811922\n"
     ]
    }
   ],
   "source": [
    "print(metrics.log_loss(eval_y, prob_xgb)) #  0.443201485761 0.443116811922"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "EXT = ensemble.ExtraTreesClassifier(n_estimators=1200, max_features= 8, criterion= 'entropy',min_samples_split= 2,\n",
    "                            max_depth= 30, min_samples_leaf= 2, n_jobs = -1,random_state=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
       "           max_depth=30, max_features=8, max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=2,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=1200, n_jobs=-1, oob_score=False, random_state=1,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXT.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prob_ext = EXT.predict_proba(eval_X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.470208956498\n"
     ]
    }
   ],
   "source": [
    "print(metrics.log_loss(eval_y, prob_ext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clfs = [XGBClassifier(learning_rate =0.01, n_estimators=2000, max_depth=8, min_child_weight=1.5, gamma=0, reg_alpha=0.01,\n",
    "                      subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1),\n",
    "        ensemble.ExtraTreesClassifier(n_estimators=1000, max_features= 8, criterion= 'entropy',min_samples_split= 4,\n",
    "                                      max_depth= 35, min_samples_leaf= 2, n_jobs = -1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train and test sets for stacking.\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating train and test sets for stacking.\")\n",
    "train_stack = np.zeros((train_X.shape[0], len(clfs)))\n",
    "test_stack = np.zeros((eval_X.shape[0], len(clfs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.01, max_delta_step=0, max_depth=8,\n",
      "       min_child_weight=1.5, missing=None, n_estimators=2000, nthread=4,\n",
      "       objective='binary:logistic', reg_alpha=0.01, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "1 ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=35, max_features=8, max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=2,\n",
      "           min_samples_split=4, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=1000, n_jobs=-1, oob_score=False,\n",
      "           random_state=None, verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "sfk = StratifiedKFold(n_splits=5) \n",
    "for j, clf in enumerate(clfs):\n",
    "    print(j, clf)\n",
    "    test_stack_j = np.zeros((eval_X.shape[0], 5))\n",
    "    folds = sfk.split(train_X, train_y)\n",
    "    for i, (train_idx, test_idx) in enumerate(folds):\n",
    "        print(\"Fold\", i)\n",
    "        X_train = train_X.iloc[train_idx,:]\n",
    "        y_train = train_y[train_idx]\n",
    "        X_test = train_X.iloc[test_idx,:]\n",
    "        y_test = train_y[test_idx]\n",
    "        if j == 0: \n",
    "            clf.fit(X_train, y_train, eval_metric='logloss')\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "        y_prob = clf.predict_proba(X_test)[:,1]\n",
    "        train_stack[test_idx, j] = y_prob\n",
    "        test_stack_j[:, i] = clf.predict_proba(eval_X)[:,1]\n",
    "    test_stack[:,j] = test_stack_j.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking...\n"
     ]
    }
   ],
   "source": [
    "print(\"Stacking...\")\n",
    "#clf = linear_model.LogisticRegression()\n",
    "clf = XGBClassifier(learning_rate =0.1, n_estimators=150, max_depth=8,\n",
    "                    min_child_weight=1.5, gamma=0, reg_alpha=0.01, subsample=0.8, \n",
    "                    colsample_bytree=0.8, objective= 'binary:logistic',\n",
    "                    nthread=4, scale_pos_weight=1)\n",
    "clf.fit(train_stack, train_y, eval_metric='logloss')\n",
    "prob = clf.predict_proba(test_stack)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.468157543116\n"
     ]
    }
   ],
   "source": [
    "print(metrics.log_loss(eval_y, prob))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
