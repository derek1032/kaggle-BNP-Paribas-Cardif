{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### This notebook is for submitting predictions to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model \n",
    "from itertools import combinations\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test= pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114321 observations and 131 features\n"
     ]
    }
   ],
   "source": [
    "print(\"{} observations and {} features\".format(train.shape[0], train.shape[1] - 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Select variables with higher correlations against the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "var = ['target', 'v10', 'v12', 'v14', 'v21', 'v22', 'v24', 'v30', 'v31', 'v34', 'v38', 'v40', 'v47', 'v50', 'v52', 'v56', 'v62', \n",
    "'v66', 'v72', 'v74', 'v75', 'v79', 'v107','v112', 'v113', 'v114', 'v129']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 duplicates\n"
     ]
    }
   ],
   "source": [
    "print('%d duplicates' % sum(train.duplicated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select the variables with higher correlations against the target variable\n",
    "train_X = train.loc[:,var]\n",
    "test_X = test.loc[:,var[1:]]\n",
    "train_y = train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat_cols = train_X.select_dtypes(include=['O']).columns\n",
    "num_cols = train_X.select_dtypes(include=['int', 'float']).columns[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Dealing  with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_X.loc[:,num_cols]=train_X.loc[:,num_cols].fillna(-1) # Fill NaN with -1\n",
    "train_X.loc[:,cat_cols]=train_X.loc[:,cat_cols].fillna('99') # Fill NaN with '99' for categorical columns\n",
    "test_X.loc[:,num_cols]=test_X.loc[:,num_cols].fillna(-1) \n",
    "test_X.loc[:,cat_cols]=test_X.loc[:,cat_cols].fillna('99') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Converting some numerical variables back to integers rounding numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_X.loc[:,'v10'] = round(train_X.loc[:,'v10'] / 0.02188170970971, 2)\n",
    "train_X.loc[:,'v40'] = round(train_X.loc[:,'v40'] / 0.000723536569601, 2)\n",
    "train_X.loc[:,'v50'] = round(train_X.loc[:,'v50'] / 0.00146724324324, 2)\n",
    "test_X.loc[:,'v10'] = round(test_X.loc[:,'v10'] / 0.02188170970971, 2)\n",
    "test_X.loc[:,'v40'] = round(test_X.loc[:,'v40'] / 0.000723536569601, 2)\n",
    "test_X.loc[:,'v50'] = round(test_X.loc[:,'v50'] / 0.00146724324324, 2)\n",
    "\n",
    "train_X = round(train_X, 2)\n",
    "test_X = round(test_X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Creating new variables for capturing the interactions among categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# for c in cat_cols[1:]:\n",
    "#     train.loc[:, 'v22'+c] = train_X.loc[:, 'v22'] + train_X.loc[:, c]\n",
    "#     test.loc[:, 'v22'+c] = test_X.loc[:, 'v22'] + test_X.loc[:, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for c in combinations(cat_cols, 2):\n",
    "    train_X.loc[:,c[0]+c[1]] = train_X.loc[:,c[0]] + train_X.loc[:,c[1]]\n",
    "    test_X.loc[:,c[0]+c[1]] = test_X.loc[:,c[0]] + test_X.loc[:,c[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for c in combinations(cat_cols[1:], 2):\n",
    "    train_X.loc[:,'v22'+c[0]+c[1]] = train_X.loc[:,'v22'] + train_X.loc[:,c[0]] + train_X.loc[:,c[1]]\n",
    "    test_X.loc[:,'v22'+c[0]+c[1]] = test_X.loc[:,'v22'] + test_X.loc[:,c[0]] + test_X.loc[:,c[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_X.loc[:, 'v22'+'v40-v50'] = train_X.loc[:, 'v22'] + (train_X.loc[:,'v40']-train_X.loc[:,'v50']).map(str)\n",
    "test_X.loc[:, 'v22'+'v40-v50'] = test_X.loc[:, 'v22'] + (test_X.loc[:,'v40']-test_X.loc[:,'v50']).map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Categorical Encoding Using Target Statistics \n",
    "Transform individual value of a categorical attribute $X$ to a scalar, $S_i$, representing the estimate of probability that the target variable equals to 1 given $X=X_i$. $S_i$ is calculated using the formula: $$S_i=\\beta(n_i)\\frac{n_{iy}}{n_i}+(1-\\beta(n_i))\\frac{n_y}{n_{TR}}$$ \n",
    "where $n_y$ is total number of cases such that $y=1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "k = 1.5\n",
    "f = 0.5\n",
    "pt = sum(train_X.target)/len(train_X) # the prior probability of the dependent attribute\n",
    "def beta(x, count, k, f):\n",
    "    ni = 0 if x not in count else count[x]\n",
    "    return 1 / (1 + np.exp(-(ni-k)/f))\n",
    "\n",
    "def transform_to_scalar(x, mean, pt, beta):\n",
    "    pi = pt if x not in mean else mean[x]\n",
    "    return (beta * pi + (1 - beta) * pt) # * np.random.normal(1, 0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Leave-One-Out Encoding Using Target Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cat_cols = train_X.select_dtypes(include=['O']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_rows = train_X.shape[0]\n",
    "train_X = train_X.reset_index(drop=True)\n",
    "train_y = train_y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    mean = train_X.groupby([col])['target'].mean()\n",
    "    count = train_X.groupby([col])['target'].count()\n",
    "    test_X.loc[:, col] = test_X.loc[:,col].apply(lambda x: transform_to_scalar(x, mean, pt, beta(x, count, k, f)))\n",
    "    for i in range(4):\n",
    "        fold = range(i, n_rows, 4)\n",
    "        mean = train_X.drop(fold, axis=0).groupby([col])['target'].mean()\n",
    "        count = train_X.drop(fold, axis=0).groupby([col])['target'].count()\n",
    "        train_X.loc[fold, col] = train_X.loc[fold,col].apply(lambda x: transform_to_scalar(x, mean, pt, beta(x, count, k, f)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_X = train_X.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_X[cat_cols] = train_X[cat_cols].astype('float')\n",
    "test_X[cat_cols] = test_X[cat_cols].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "XGB = XGBClassifier(learning_rate =0.01, n_estimators=2000, max_depth=8,\n",
    " min_child_weight=1.5, gamma=0, reg_alpha=0.01, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic',\n",
    " nthread=4, scale_pos_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0, learning_rate=0.01, max_delta_step=0, max_depth=8,\n",
       "       min_child_weight=1.5, missing=None, n_estimators=2000, nthread=4,\n",
       "       objective='binary:logistic', reg_alpha=0.01, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB.fit(train_X, train_y, eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.21000093,  0.96492577,  0.86594748, ...,  0.88996011,\n",
       "        0.93076468,  0.48586789], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_xgb = XGB.predict_proba(test_X)[:,1]; prob_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.21000093,  0.96492577,  0.86594748, ...,  0.88996011,\n",
       "        0.93076468,  0.48586789], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Stacking XGBoost and ExtraTreesClassifier for generalizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clfs = [XGBClassifier(learning_rate =0.05, n_estimators=1000, max_depth=6, min_child_weight=1.5, gamma=0, reg_alpha=0.01,\n",
    "                      subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1),\n",
    "        XGBClassifier(learning_rate =0.01, n_estimators=2000, max_depth=8, min_child_weight=1.5, gamma=0, reg_alpha=0.01,\n",
    "                      subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1),\n",
    "        ensemble.ExtraTreesClassifier(n_estimators=1000, max_features= 6, criterion= 'entropy',min_samples_split= 4,\n",
    "                                      max_depth= 25, min_samples_leaf= 2, n_jobs = -1),\n",
    "        ensemble.ExtraTreesClassifier(n_estimators=1200, max_features= 8, criterion= 'entropy',min_samples_split= 4,\n",
    "                                      max_depth= 35, min_samples_leaf= 2, n_jobs = -1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train and test sets for stacking.\n"
     ]
    }
   ],
   "source": [
    "# Creating train and test sets for stacking\n",
    "train_stack = np.zeros((train_X.shape[0], len(clfs)))\n",
    "test_stack = np.zeros((test_X.shape[0], len(clfs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
      "       gamma=0, learning_rate=0.01, max_delta_step=0, max_depth=8,\n",
      "       min_child_weight=1.5, missing=None, n_estimators=2000, nthread=4,\n",
      "       objective='binary:logistic', reg_alpha=0.01, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "1 ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=35, max_features=8, max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=2,\n",
      "           min_samples_split=4, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=1000, n_jobs=-1, oob_score=False,\n",
      "           random_state=None, verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "sfk = StratifiedKFold(n_splits=5) \n",
    "for j, clf in enumerate(clfs):\n",
    "    print(j+1, clf)\n",
    "    test_stack_j = np.zeros((test_X.shape[0], 5))\n",
    "    folds = sfk.split(train_X, train_y)\n",
    "    for i, (train_idx, test_idx) in enumerate(folds):\n",
    "        print(\"Fold\", i+1)\n",
    "        X_train = train_X.iloc[train_idx,:]\n",
    "        y_train = train_y[train_idx]\n",
    "        X_test = train_X.iloc[test_idx,:]\n",
    "        y_test = train_y[test_idx]\n",
    "        if j == 0: \n",
    "            clf.fit(X_train, y_train, eval_metric='logloss')\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "        y_prob = clf.predict_proba(X_test)[:,1]\n",
    "        train_stack[test_idx, j] = y_prob\n",
    "        test_stack_j[:, i] = clf.predict_proba(test_X)[:,1]\n",
    "    test_stack[:,j] = test_stack_j.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking...\n"
     ]
    }
   ],
   "source": [
    "print(\"Stacking...\")\n",
    "#clf = linear_model.LogisticRegression()\n",
    "clf = XGBClassifier(learning_rate =0.1, n_estimators=150, max_depth=8,\n",
    "                    min_child_weight=1.5, gamma=0, reg_alpha=0.01, subsample=0.8, \n",
    "                    colsample_bytree=0.8, objective= 'binary:logistic',\n",
    "                    nthread=4, scale_pos_weight=1)\n",
    "clf.fit(train_stack, train_y, eval_metric='logloss')\n",
    "prob = clf.predict_proba(test_stack)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'ID':test.ID, 'PredictedProb':prob_xgb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'ID':test.ID, 'PredictedProb':prob})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "1. A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
